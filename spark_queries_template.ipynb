{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181bbe29-26f4-4c4a-a9e9-0053263bff52",
   "metadata": {},
   "source": [
    "# Assignment: Scalable Processing\n",
    "## Yelp Reviews and Authenticity\n",
    "\n",
    "Large Scale Data Analysis | by Maciej Jalocha | macja@itu.dk | 10.03.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92690678-bb97-43d7-ab27-4a6f8db87079",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Connecting to the Spark Cluster job using the two JobParameters.json\n",
    "\n",
    "To connect this jupyter notebook with your Spark cluster, we need to tell jupyter how it can access the spark cluster. Below code accomplishes that. Do not worry about how it works, just run the cell once to connect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d483a774-a0ca-4873-a2ee-51d2fcd30ead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:18:22.130606Z",
     "iopub.status.busy": "2025-03-28T16:18:22.129869Z",
     "iopub.status.idle": "2025-03-28T16:19:08.333757Z",
     "shell.execute_reply": "2025-03-28T16:19:08.331626Z",
     "shell.execute_reply.started": "2025-03-28T16:18:22.130528Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "alert(\"Warning: Found these files [] that should (most likely) be moved inside your Home folder. Make sure your Git repository and notebooks are all saved inside your Home folder and not at the 'root'/top of filesystem. Please move your files to prevent them from disappearing.\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell has not been executed before. Please restart the UCloud jobs if any error message pops up. Running setup cell now.\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# DO NOT CHANGE ANYTHING HERE.\n",
    "# IF YOU HAVE PROBLEMS, CHECK THE ASSIGNMENT GUIDE CAREFULLY \n",
    "#####################################################################\n",
    "from IPython.display import Javascript, display\n",
    "import jupyterlab\n",
    "import os, json, pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "\n",
    "def show_popup(message):\n",
    "    display(Javascript(f'alert(\"{message}\")'))\n",
    "\n",
    "def check_correct_file_location():\n",
    "    items = os.listdir('/work')\n",
    "    items_expected = ['yelp', 'Home','JobParameters.json', 'emails']\n",
    "    if sorted(items) != sorted(items_expected):\n",
    "        items_to_be_moved = [item for item in items if item not in items_expected and item[0] != '.'] # Ignore hidden files starting with .\n",
    "        show_popup(f\"Warning: Found these files {items_to_be_moved} that should (most likely) be moved inside your Home folder. Make sure your Git repository and notebooks are all saved inside your Home folder and not at the 'root'/top of filesystem. Please move your files to prevent them from disappearing.\")\n",
    "    if 'emails' not in items_expected:\n",
    "        show_popup(f'Error: the folder \"emails\" does not seem to be accessible - did you remeber to add it to the Spark Cluster job and JupyterLab job?')\n",
    "    \n",
    "check_correct_file_location()\n",
    "\n",
    "SUPPORTED_SPARK_VERSION = \"3.3.1\"\n",
    "SUPPORTED_JUPYTERLAB_VERSION = \"3.5.1\"\n",
    "if jupyterlab.__version__ != SUPPORTED_JUPYTERLAB_VERSION:\n",
    "    show_popup(f\"Wrong JupyterLab version :( When starting the UCloud job you selected {jupyterlab.__version__} but it should have been {SUPPORTED_JUPYTERLAB_VERSION}\")\n",
    "    show_popup(\"Please shutdown this JupyterLab job and follow the instructions carefully in the UCloud setup guide PDF on LearnIT\") \n",
    "elif '_EXECUTED_' in globals(): # Only execute this cell once.\n",
    "    # check if variable '_EXECUTED_' exists in the global variable namespace\n",
    "    print(\"Already been executed once, not running again!\")\n",
    "else:\n",
    "    print(\"Cell has not been executed before. Please restart the UCloud jobs if any error message pops up. Running setup cell now.\")\n",
    "    # Two files are automatically read: JobParameters.json for the Spark Cluster job using a temporary spark instance\n",
    "    # and JobParameters.json for the Jupyter Lab job to extract the hostname of the cluster. \n",
    "\n",
    "    MASTER_HOST_NAME = None\n",
    "\n",
    "    # Open the parameters Jupyter Lab app was launched with\n",
    "    with open('/work/JobParameters.json', 'r') as file:\n",
    "        JUPYTER_LAB_JOB_PARAMS = json.load(file)\n",
    "        # from pprint import pprint; pprint(JUPYTER_LAB_JOB_PARAMS) \n",
    "        for resource in JUPYTER_LAB_JOB_PARAMS['request']['resources']:\n",
    "            if 'hostname' in resource.keys():\n",
    "                MASTER_HOST_NAME = resource['hostname']\n",
    "    \n",
    "    if MASTER_HOST_NAME != \"spark-cluster\":\n",
    "        msg = f\"The JupyterLab job was started using spark hostname {MASTER_HOST_NAME}. This is not recommended, please start it using spark-cluster instead\"\n",
    "        show_popup(msg)\n",
    "        print(msg)\n",
    "    else:\n",
    "        MASTER_HOST = f\"spark://{MASTER_HOST_NAME}:7077\"\n",
    "\n",
    "        conf = SparkConf().setAll([\n",
    "                (\"spark.app.name\", 'reading_job_params_app'), \n",
    "                (\"spark.master\", MASTER_HOST),\n",
    "            ])\n",
    "\n",
    "        spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                    .getOrCreate()\n",
    "        \n",
    "        if spark.version != SUPPORTED_SPARK_VERSION:\n",
    "            show_popup(f\"Wrong Spark Cluster version :( When starting the UCloud job you selected {spark.version} but it should have been {SUPPORTED_SPARK_VERSION}\")\n",
    "            show_popup(\"Please shutdown this JupyterLab job, the Spark Cluster and follow the instructions carefully in the UCloud setup guide PDF on LearnIT\") \n",
    "\n",
    "        CLUSTER_PARAMETERS_JSON_DF = spark.read.option(\"multiline\",\"true\").json('/work/JobParameters.json')\n",
    "        \n",
    "        # Extract cluster info from the specific JobParameters.json\n",
    "        NODES = CLUSTER_PARAMETERS_JSON_DF.select(\"request.replicas\").first()[0]\n",
    "        CPUS_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.cpu\").first()[0] - 1\n",
    "        MEM_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.memoryInGigs\").first()[0]\n",
    "\n",
    "        CLUSTER_CORES_MAX = CPUS_PER_NODE * NODES\n",
    "        CLUSTER_MEMORY_MAX = MEM_PER_NODE * NODES \n",
    "        \n",
    "        if CPUS_PER_NODE > 1:\n",
    "            EXECUTOR_CORES = CPUS_PER_NODE - 1  # set cores per executor on worker node\n",
    "        else:\n",
    "            EXECUTOR_CORES = CPUS_PER_NODE \n",
    "\n",
    "        try:\n",
    "            EXECUTOR_MEMORY = int(\n",
    "                MEM_PER_NODE / (CPUS_PER_NODE / EXECUTOR_CORES) * 0.5\n",
    "            )  # set executor memory in GB on each worker node\n",
    "        except ZeroDivisionError:\n",
    "            show_popup(f\"Please make sure you selected 3 nodes for the Spark Cluster, each with 24 GB of ram. You selected {MEM_PER_NODE} GB ram and {NODES} node(s)\")\n",
    "            \n",
    "        # Make sure there is a dir for spark logs\n",
    "        if not os.path.exists('spark_logs'):\n",
    "            os.mkdir('spark_logs')\n",
    "\n",
    "        conf = SparkConf().setAll(\n",
    "            [\n",
    "                (\"spark.app.name\", 'spark_assignment'), # Change to your liking \n",
    "                (\"spark.sql.caseSensitive\", False), # Optional: Make queries strings sensitive to captialization\n",
    "                (\"spark.master\", MASTER_HOST),\n",
    "                (\"spark.cores.max\", CLUSTER_CORES_MAX),\n",
    "                (\"spark.executor.cores\", EXECUTOR_CORES),\n",
    "                (\"spark.executor.memory\", str(EXECUTOR_MEMORY) + \"g\"),\n",
    "                (\"spark.eventLog.enabled\", True),\n",
    "                (\"spark.eventLog.dir\", \"spark_logs\"),\n",
    "                (\"spark.history.fs.logDirectory\", \"spark_logs\"),\n",
    "                (\"spark.deploy.mode\", \"cluster\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ## check executor memory, taking into accout 10% of memory overhead (minimum 384 MiB)\n",
    "        CHECK = (CLUSTER_CORES_MAX / EXECUTOR_CORES) * (\n",
    "            EXECUTOR_MEMORY + max(EXECUTOR_MEMORY * 0.10, 0.403)\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            int(CHECK) <= CLUSTER_MEMORY_MAX\n",
    "        ), \"Executor memory larger than cluster total memory!\"\n",
    "\n",
    "        # Stop previous session that was just for loading cluster params\n",
    "        spark.stop()\n",
    "\n",
    "        # Start new session with above config, that has better resource handling\n",
    "        spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                    .getOrCreate()\n",
    "        sc = spark.sparkContext\n",
    "        _EXECUTED_ = True\n",
    "        print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbb4de-8b40-4364-b8d3-2cc488b16cbf",
   "metadata": {},
   "source": [
    "Click on the \"SparkMonitor\" tab at the top in Jupyter Lab to see the status of running code on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de3dec-d95c-44b9-a996-12e97cc34c2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the data\n",
    "Here we specify where the yelp datasets are located on UCloud and read then using the spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa96702-5902-4482-9cd2-77d6f5da0f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:20:46.927472Z",
     "iopub.status.busy": "2025-03-28T16:20:46.927094Z",
     "iopub.status.idle": "2025-03-28T16:21:00.910507Z",
     "shell.execute_reply": "2025-03-28T16:21:00.909232Z",
     "shell.execute_reply.started": "2025-03-28T16:20:46.927444Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the business and review files\n",
    "# This is the path to the shared datasets provided by adding an the dataset input folder\n",
    "# when submitting the spark cluster job.\n",
    "business = spark.read.json('file:////work/yelp/yelp_academic_dataset_business.json') # Use the file:/// prefix to indicate we want to read from the cluster's filesystem\n",
    "business = business.persist()\n",
    "# Persist 2 commonly used dataframes since they're used for later computations\n",
    "# https://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/\n",
    "\n",
    "users = spark.read.json(\"file:////work/yelp/yelp_academic_dataset_user.json\")\n",
    "\n",
    "reviews = spark.read.json('file:////work/yelp/yelp_academic_dataset_review.json')\n",
    "reviews = reviews.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a303b5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PySpark example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235e5257-1a0e-4189-87fc-075ff2858c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:19:50.873247Z",
     "iopub.status.busy": "2025-03-28T16:19:50.872446Z",
     "iopub.status.idle": "2025-03-28T16:19:50.882238Z",
     "shell.execute_reply": "2025-03-28T16:19:50.880966Z",
     "shell.execute_reply.started": "2025-03-28T16:19:50.873192Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business_id',\n",
       " 'cool',\n",
       " 'date',\n",
       " 'funny',\n",
       " 'review_id',\n",
       " 'stars',\n",
       " 'text',\n",
       " 'useful',\n",
       " 'user_id']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show PySpark dataframes:\n",
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9630e-1dd0-4abf-9952-bd7e252a8061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "business.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f6184-410e-4a04-a4a1-7b080353dc1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of rows with no sampling:\n",
    "reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31d1a2-3571-49cd-af5a-98d6d23027de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPTIONAL:\n",
    "# Reduce resource usage and make queries run faster\n",
    "# by only using a small sample of the dataframe\n",
    "# and overwriting previous variable \"df\".\n",
    "# Useful while developing, not so much to\n",
    "# provide final answers. Therefore: Remember to \n",
    "# to re-read the df when done developing code using\n",
    "# df = spark.read etc like above.\n",
    "\n",
    "\n",
    "# Get number of rows after sampling:\n",
    "reviews.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20087cdb-d6f2-4aab-9fe2-bf3f072044db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "business.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d258fa6-1402-4c3a-a252-a9dbfd4a87d9",
   "metadata": {},
   "source": [
    "Example: Say we're only interested in reviews of good mexican restaurants in Arizona. You can delete this when you do your own thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3071b-2f83-4afe-8a37-fe7356757c0e",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07983acb-15d8-45ea-9d12-3bc8759280b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter to only Arizona businesses with \"Mexican\" as part of their categories\n",
    "az_mex = business.filter(business.state == \"AZ\")\\\n",
    "                .filter(business.categories.rlike(\"Mexican\"))\\\n",
    "                .select(\"business_id\", \"name\")\n",
    "\n",
    "# Join with the reviews\n",
    "az_mex_rs = reviews.join(az_mex, on=\"business_id\", how=\"inner\")\n",
    "\n",
    "# Filter to only 5 star reviews\n",
    "good_az_mex_rs = az_mex_rs.filter(az_mex_rs.stars == 5)\\\n",
    "                        .select(\"name\",\"text\")\n",
    "\n",
    "# Print the top 20 rows of the DataFrame\n",
    "good_az_mex_rs.show()\n",
    "\n",
    "# Convert to pandas (local object) and save to local file system\n",
    "good_az_mex_rs.toPandas().to_csv(\"good_az_reviews.csv\", header=True, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d265d-cba4-4ade-851e-dad67771864b",
   "metadata": {},
   "source": [
    "See assignment PDF for task descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a624efd-902b-4b3a-bda4-0af2c2a27fee",
   "metadata": {},
   "source": [
    "### Samplling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884f3ce-cfdc-4617-8ff8-8b0b59185bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.sample(withReplacement=False, fraction=25/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08a2d0-fba5-402d-bb87-448422312058",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b394058",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3.1.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee564c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:00:02.788522Z",
     "iopub.status.busy": "2025-03-24T15:00:02.787626Z",
     "iopub.status.idle": "2025-03-24T15:00:03.017043Z",
     "shell.execute_reply": "2025-03-24T15:00:03.015857Z",
     "shell.execute_reply.started": "2025-03-24T15:00:02.788463Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150346"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here...\n",
    "business.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647cf67c-2499-4c76-9aa9-a2815c322276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "business.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a546fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3.1.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aca680c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:01:05.721040Z",
     "iopub.status.busy": "2025-03-24T15:01:05.720289Z",
     "iopub.status.idle": "2025-03-24T15:01:06.020820Z",
     "shell.execute_reply": "2025-03-24T15:01:06.018910Z",
     "shell.execute_reply.started": "2025-03-24T15:01:05.720983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "respected_businesses=business.filter(business.stars >= 5).filter(business.review_count >= 500).select(\"*\")\n",
    "respected_businesses.toPandas().to_csv(\"3_1_2_respected_businesses.csv\", header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e2a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3.1.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a849fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:21:11.600434Z",
     "iopub.status.busy": "2025-03-28T16:21:11.599627Z",
     "iopub.status.idle": "2025-03-28T16:21:19.903533Z",
     "shell.execute_reply": "2025-03-28T16:21:19.902469Z",
     "shell.execute_reply.started": "2025-03-28T16:21:11.600385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "influencers = users.filter(users.review_count >= 1000).select(\"user_id\")\n",
    "influencers.toPandas().to_csv(\"3_1_3_influencers.csv\", header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773c064",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3.1.4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0d9d0-e011-4e3a-bdaa-7270dcfe83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb504f36-b931-45aa-a223-703120717228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:20:38.201944Z",
     "iopub.status.busy": "2025-03-28T16:20:38.200891Z",
     "iopub.status.idle": "2025-03-28T16:20:38.223906Z",
     "shell.execute_reply": "2025-03-28T16:20:38.222684Z",
     "shell.execute_reply.started": "2025-03-28T16:20:38.201871Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "\n",
    "reviews = reviews.sample(withReplacement=False, fraction=1/50)\n",
    "business = business.sample(withReplacement=False, fraction=1/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4cdfdb-dbfa-49a7-b64a-c3d4af91416a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T16:22:01.068618Z",
     "iopub.status.busy": "2025-03-28T16:22:01.067993Z",
     "iopub.status.idle": "2025-03-28T16:22:10.063720Z",
     "shell.execute_reply": "2025-03-28T16:22:10.062999Z",
     "shell.execute_reply.started": "2025-03-28T16:22:01.068576Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "counts = business\\\n",
    ".join(reviews, on=\"business_id\", how=\"inner\")\\\n",
    ".join(influencers, on=\"user_id\", how=\"inner\")\\\n",
    ".select(\"name\", \"user_id\")\\\n",
    ".groupBy('name')\\\n",
    ".count()\\\n",
    ".filter(col('count') >= 5)\\\n",
    ".toPandas().to_csv(\"3_1_4_occupied_businesses.csv\", header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922a694",
   "metadata": {},
   "source": [
    "### Task 3.1.5: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f8cb8-8bb5-42dc-a45e-82cb4100f1d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:44:50.164801Z",
     "iopub.status.busy": "2025-03-24T14:44:50.164108Z",
     "iopub.status.idle": "2025-03-24T14:44:50.173820Z",
     "shell.execute_reply": "2025-03-24T14:44:50.172216Z",
     "shell.execute_reply.started": "2025-03-24T14:44:50.164714Z"
    },
    "tags": []
   },
   "source": [
    "Find anordered list of users based on the average star counts they have given in all their reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64233be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:04:06.902651Z",
     "iopub.status.busy": "2025-03-24T15:04:06.901834Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "from pyspark.sql.functions import col\n",
    "counts = reviews\\\n",
    ".join(users, on=\"user_id\", how=\"right\")\\\n",
    ".select(\"stars\", \"user_id\")\\\n",
    ".groupBy('user_id')\\\n",
    ".mean()\\\n",
    ".sort(col('avg(stars)'), ascending=False)\\\n",
    ".show()\n",
    "# .toPandas().to_csv(\"3_1_5_ordered_users.csv\", header=True, index=False, encoding='utf-8')\n",
    "# too big to save haha.\n",
    "#include all users, including with no corresponding reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6bec7",
   "metadata": {},
   "source": [
    "### Task 3.2.1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eadc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517b579",
   "metadata": {},
   "source": [
    "### Task 3.2.2: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7c1d5",
   "metadata": {},
   "source": [
    "### Task 3.3: Building a Rating Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
